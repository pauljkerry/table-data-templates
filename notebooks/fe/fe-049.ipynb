{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dede9c68-9c41-4f24-a5bc-1b139dd714fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T05:46:39.046466Z",
     "iopub.status.busy": "2025-10-05T05:46:39.046297Z",
     "iopub.status.idle": "2025-10-05T05:46:39.575718Z",
     "shell.execute_reply": "2025-10-05T05:46:39.575336Z",
     "shell.execute_reply.started": "2025-10-05T05:46:39.046455Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "from collections import Counter\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "\n",
    "import ipynbname\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../..\"))\n",
    "\n",
    "from src.utils.target_encoding import target_encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e2ee8-c192-41a6-965f-af38fa23bc87",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36d6ad7a-72fe-4727-b888-bf3095c0d43a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T05:46:39.576269Z",
     "iopub.status.busy": "2025-10-05T05:46:39.576134Z",
     "iopub.status.idle": "2025-10-05T05:46:39.587555Z",
     "shell.execute_reply": "2025-10-05T05:46:39.587100Z",
     "shell.execute_reply.started": "2025-10-05T05:46:39.576260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem = ipynbname.path().stem\n",
    "m = re.search(r\"(\\d+)$\", stem)\n",
    "\n",
    "ID = m.group(1)\n",
    "SEED = 42\n",
    "LEVEL = \"l1\"\n",
    "ALPHA = 3\n",
    "FEATURE_DIR = Path(f\"../../artifacts/features/{ID}\")\n",
    "\n",
    "os.makedirs(FEATURE_DIR, exist_ok=True)\n",
    "\n",
    "pl.Config.set_tbl_rows(500)\n",
    "pl.Config.set_tbl_cols(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209e14a-75ee-45ae-8596-16f1b7a5551d",
   "metadata": {},
   "source": [
    "### Urils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bae812c-9059-4f2b-b7de-b9f5e63f76fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T05:46:39.588044Z",
     "iopub.status.busy": "2025-10-05T05:46:39.587941Z",
     "iopub.status.idle": "2025-10-05T05:46:39.678884Z",
     "shell.execute_reply": "2025-10-05T05:46:39.678414Z",
     "shell.execute_reply.started": "2025-10-05T05:46:39.588036Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_info(\n",
    "    train: pl.DataFrame,\n",
    "    test: pl.DataFrame\n",
    ") -> tuple[float, float, float]:\n",
    "    train_mem = sum(train[col].to_numpy().nbytes for col in train.columns) / 1024**3\n",
    "    test_mem = sum(test[col].to_numpy().nbytes for col in test.columns) / 1024**3\n",
    "\n",
    "    print(\"=== Shape & Memory ===\")\n",
    "    print(f\"Train Shape: {train.shape}, Test Shape: {test.shape}\")\n",
    "    print(f\"Train Memory: {train_mem:.2f} GB, Test Memory: {test_mem:.2f} GB\\n\")\n",
    "\n",
    "    dtype_counts = Counter([str(dt) for dt in train.dtypes])\n",
    "\n",
    "    n_cat = None\n",
    "    print(\"=== DTypes ===\")\n",
    "    for dtype, cnt in dtype_counts.items():\n",
    "        print(f\"{dtype}: {cnt}\")\n",
    "        if dtype == \"Categorical\":\n",
    "            n_cat = cnt\n",
    "    return train_mem, test_mem, n_cat\n",
    "\n",
    "\n",
    "def downcast(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    INT32_MIN, INT32_MAX = -2_147_483_648, 2_147_483_647\n",
    "\n",
    "    df = df.with_columns(pl.col(pl.Float64).cast(pl.Float32))\n",
    "\n",
    "    # Int64で安全に落とせる列だけ選別\n",
    "    int64_cols = [c for c, dt in df.schema.items() if dt == pl.Int64]\n",
    "    safe_cols = []\n",
    "    for c in int64_cols:\n",
    "        mn, mx = df[c].min(), df[c].max()\n",
    "        if mn >= INT32_MIN and mx <= INT32_MAX:\n",
    "            safe_cols.append(c)\n",
    "\n",
    "    # 安全な列だけ Int32 に\n",
    "    if safe_cols:\n",
    "        df = df.with_columns(pl.col(safe_cols).cast(pl.Int32))\n",
    "    return df\n",
    "\n",
    "\n",
    "def target_encoding_orig(\n",
    "    tr_df: pl.DataFrame,\n",
    "    test_df: pl.DataFrame,\n",
    "    orig_df: pl.DataFrame,\n",
    "    key_cols: list[str],\n",
    "    target: str = \"target\",\n",
    "    stats: tuple[str, ...] = (\"mean\", \"std\", \"min\", \"max\", \"median\", \"count\"),\n",
    "    alpha: int = 20,\n",
    "    n_splits: int = 5,\n",
    "    seed: int = 42\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Out-of-fold (OOF) target encoding with M-estimate smoothing.\n",
    "\n",
    "    For each column in `key_cols`, per-category statistics are computed on the\n",
    "    fold's training split and joined to the validation split (leak-free).\n",
    "    Test features are computed per fold and averaged. For \"mean\", the smoothed\n",
    "    estimate is:\n",
    "        (n * mean + alpha * global_mean) / (n + alpha)\n",
    "    Unseen categories are filled with the fold's global statistics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tr_df : pl.DataFrame\n",
    "        Training data. Must contain `target` and all `key_cols`.\n",
    "    test_df : pl.DataFrame\n",
    "        Unlabeled data. Must contain all `key_cols`.\n",
    "    key_cols : list[str]\n",
    "        Discrete/categorical keys used for grouping. Do not pass raw float\n",
    "        columns; round/bin or stringify them first to avoid join drift.\n",
    "    target : str, default \"target\"\n",
    "        Target column name. For \"count\", this function counts positives as\n",
    "        `(target == 1).sum()` (binary assumption).\n",
    "    stats : tuple of {\"mean\",\"std\",\"min\",\"max\",\"median\",\"count\"}, default (...)\n",
    "        Per-category statistics to output. Only \"mean\" is smoothed by `alpha`.\n",
    "        (\"count\" means positive count for binary targets.)\n",
    "    alpha : float, default 20.0\n",
    "        Smoothing strength (half-life). `alpha=0` disables smoothing.\n",
    "        n≈alpha ⇒ the category mean is trusted ~50%.\n",
    "    n_splits : int, default 5\n",
    "        Number of StratifiedKFold splits.\n",
    "    seed : int, default 42\n",
    "        Random seed for fold shuffling.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pl.DataFrame\n",
    "        Encoded features for train and test stacked vertically. Columns are\n",
    "        named `{target}_{stat}_by_{col}` in the order of `key_cols`.\n",
    "        Shape: (tr_df.height + test_df.height, sum_over_cols len(stats_for_col))\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - OOF computation prevents target leakage.\n",
    "    - Unseen categories are filled with fold-wise global stats (e.g., global_mean).\n",
    "    - If you need raw frequency n_i, add an explicit aggregation; \"count\" here\n",
    "      is the positive-class count (binary).\n",
    "    \"\"\"\n",
    "    def stat_names(col: str) -> list[str]:\n",
    "        names = []\n",
    "        if \"mean\" in stats:\n",
    "            names.append(f\"orig_{target}_mean_by_{col}\")\n",
    "        if \"std\" in stats:\n",
    "            names.append(f\"orig_{target}_std_by_{col}\")\n",
    "        if \"min\" in stats:\n",
    "            names.append(f\"orig_{target}_min_by_{col}\")\n",
    "        if \"max\" in stats:\n",
    "            names.append(f\"orig_{target}_max_by_{col}\")\n",
    "        if \"median\" in stats:\n",
    "            names.append(f\"orig_{target}_median_by_{col}\")\n",
    "        if \"count\" in stats:\n",
    "            names.append(f\"orig_{target}_count_by_{col}\")  # 1の個数\n",
    "        return names\n",
    "\n",
    "    all_cols = []\n",
    "    for col in key_cols:\n",
    "        all_cols.extend(stat_names(col))\n",
    "\n",
    "    N_tr, N_te = tr_df.height, test_df.height\n",
    "\n",
    "    te_train = {c: np.zeros(N_tr, dtype=np.float32) for c in all_cols}\n",
    "    te_test = {c: np.zeros(N_te, dtype=np.float32) for c in all_cols}\n",
    "\n",
    "    train = orig_df\n",
    "    val = tr_df\n",
    "    global_mean = train.select(pl.col(target).mean()).to_series()[0]\n",
    "\n",
    "    base = train.select([\n",
    "        pl.col(target).mean().alias(\"mean\"),\n",
    "        pl.col(target).std(ddof=1).alias(\"std\"),\n",
    "        pl.col(target).min().alias(\"min\"),\n",
    "        pl.col(target).max().alias(\"max\"),\n",
    "        pl.col(target).median().alias(\"median\"),\n",
    "        (pl.col(target) == 1).sum().alias(\"cnt\"),\n",
    "    ]).to_dicts()[0]\n",
    "\n",
    "    for col in tqdm(key_cols):\n",
    "        fill_map = {}\n",
    "        for s in stats:\n",
    "            name = f\"orig_{target}_{s}_by_{col}\"\n",
    "            if s == \"count\":\n",
    "                fill_map[name] = 0.0\n",
    "            else:\n",
    "                fill_map[name] = float(base[s])\n",
    "\n",
    "        aggs = []\n",
    "        col_names = []\n",
    "        if \"mean\" in stats:\n",
    "            aggs.append(\n",
    "                (\n",
    "                    (pl.col(target).sum() + pl.lit(alpha) * pl.lit(global_mean))\n",
    "                    / (pl.len() + pl.lit(alpha))\n",
    "                ).alias(f\"orig_{target}_mean_by_{col}\")\n",
    "            )\n",
    "            col_names.append(f\"orig_{target}_mean_by_{col}\")\n",
    "        if \"std\" in stats:\n",
    "            aggs.append(\n",
    "                pl.col(target).std(ddof=1).alias(f\"orig_{target}_std_by_{col}\")\n",
    "            )\n",
    "            col_names.append(f\"orig_{target}_std_by_{col}\")\n",
    "        if \"min\" in stats:\n",
    "            aggs.append(\n",
    "                pl.col(target).min().alias(f\"orig_{target}_min_by_{col}\")\n",
    "            )\n",
    "            col_names.append(f\"orig_{target}_min_by_{col}\")\n",
    "        if \"max\" in stats:\n",
    "            aggs.append(\n",
    "                pl.col(target).max().alias(f\"orig_{target}_max_by_{col}\")\n",
    "            )\n",
    "            col_names.append(f\"orig_{target}_max_by_{col}\")\n",
    "        if \"median\" in stats:\n",
    "            aggs.append(\n",
    "                pl.col(target).median().alias(f\"orig_{target}_median_by_{col}\")\n",
    "            )\n",
    "            col_names.append(f\"orig_{target}_median_by_{col}\")\n",
    "        if \"count\" in stats:\n",
    "            aggs.append(\n",
    "                (pl.col(target) == 1).sum().alias(f\"orig_{target}_count_by_{col}\")\n",
    "            )\n",
    "            col_names.append(f\"orig_{target}_count_by_{col}\")\n",
    "\n",
    "        grouped_df = (\n",
    "            train.select([col, target])\n",
    "            .group_by(col)\n",
    "            .agg(aggs)\n",
    "        )\n",
    "\n",
    "        # validation\n",
    "        val_mat = (\n",
    "            val.join(\n",
    "                grouped_df.select(col_names + [col]),\n",
    "                on=col,\n",
    "                how=\"left\"\n",
    "            )\n",
    "            .select(col_names)\n",
    "            .with_columns(\n",
    "                [\n",
    "                    pl.col(c).fill_null(fill_map[c]).alias(c)\n",
    "                    for c in col_names\n",
    "                ]\n",
    "            )\n",
    "            .to_numpy()\n",
    "            .astype(dtype=np.float32, copy=False)\n",
    "        )\n",
    "\n",
    "        for j, name in enumerate(col_names):\n",
    "            te_train[name] = val_mat[:, j]\n",
    "\n",
    "        # test\n",
    "        test_mat = (\n",
    "            test_df.join(\n",
    "                grouped_df.select(col_names + [col]),\n",
    "                on=col,\n",
    "                how=\"left\"\n",
    "            )\n",
    "            .select(col_names)\n",
    "            .with_columns(\n",
    "                [\n",
    "                    pl.col(c).fill_null(fill_map[c]).alias(c)\n",
    "                    for c in col_names\n",
    "                ]\n",
    "            )\n",
    "            .to_numpy()\n",
    "            .astype(dtype=np.float32, copy=False)\n",
    "        )\n",
    "        for j, name in enumerate(col_names):\n",
    "            te_test[name] += test_mat[:, j]\n",
    "\n",
    "        del grouped_df, val_mat, test_mat\n",
    "    del train, val\n",
    "\n",
    "    te_tr = pl.DataFrame(te_train)\n",
    "    te_test = pl.DataFrame(te_test)\n",
    "\n",
    "    return pl.concat([te_tr, te_test], how=\"vertical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bdb64f-11ed-47b8-8186-8f887fd6e5aa",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "- 2-gram TE(mean)\n",
    "- orig Targetでも2-gram TE(mean)\n",
    "- 2-gram CE(without orig)\n",
    "- TEにSmoothingを導入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0c2f120-4143-4b9d-9b01-8306ecb51782",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T05:46:39.679664Z",
     "iopub.status.busy": "2025-10-05T05:46:39.679517Z",
     "iopub.status.idle": "2025-10-05T05:46:39.781119Z",
     "shell.execute_reply": "2025-10-05T05:46:39.780777Z",
     "shell.execute_reply.started": "2025-10-05T05:46:39.679654Z"
    }
   },
   "outputs": [],
   "source": [
    "# === Load Data ===\n",
    "train = pl.read_csv(\"../../input/train.csv\").drop(\"id\")\n",
    "test = pl.read_csv(\"../../input/test.csv\").drop(\"id\")\n",
    "orig = pl.read_parquet(\"../../input/original.parquet\")\n",
    "orig = orig.with_columns(\n",
    "    pl.when(pl.col(\"y\") == \"yes\").then(1)\n",
    "      .when(pl.col(\"y\") == \"no\").then(0)\n",
    "      .otherwise(None)\n",
    "      .alias(\"y\")\n",
    ")\n",
    "\n",
    "y_tr = train[\"y\"].cast(pl.Int8)\n",
    "y_orig = orig[\"y\"].cast(pl.Int8)\n",
    "y_merged = pl.concat([y_tr, y_orig], how=\"vertical\")\n",
    "\n",
    "train = train.drop(\"y\")\n",
    "orig = orig.drop(\"y\")\n",
    "\n",
    "CATS = [col for col in train.columns if train[col].dtype == pl.Utf8]\n",
    "NUMS = [col for col in train.columns if train[col].dtype != pl.Utf8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a0bbd85-afdf-4624-903c-880cd5005fa7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T05:46:39.781455Z",
     "iopub.status.busy": "2025-10-05T05:46:39.781360Z",
     "iopub.status.idle": "2025-10-05T05:46:39.814054Z",
     "shell.execute_reply": "2025-10-05T05:46:39.813696Z",
     "shell.execute_reply.started": "2025-10-05T05:46:39.781448Z"
    }
   },
   "outputs": [],
   "source": [
    "# === 全データを結合 ===\n",
    "all_data = pl.concat([train, test, orig], how=\"vertical\")\n",
    "cat_exprs = [\n",
    "    pl.col(c)\n",
    "    .cast(pl.Categorical)\n",
    "    .to_physical()\n",
    "    .rank(\"dense\")\n",
    "    .cast(pl.Int32)\n",
    "    .alias(c)\n",
    "    for c in CATS\n",
    "]\n",
    "num_df = all_data.select(NUMS)\n",
    "cat_df = all_data.select(\n",
    "    [pl.col(c).cast(pl.Utf8).cast(pl.Categorical) for c in CATS]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d87ef63-fecd-4e49-986a-cfe95fd92f03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T05:46:39.814434Z",
     "iopub.status.busy": "2025-10-05T05:46:39.814349Z",
     "iopub.status.idle": "2025-10-05T05:46:39.990423Z",
     "shell.execute_reply": "2025-10-05T05:46:39.989942Z",
     "shell.execute_reply.started": "2025-10-05T05:46:39.814426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'job': 12, 'marital': 3, 'education': 4, 'default': 2, 'housing': 2, 'loan': 2, 'contact': 3, 'month': 12, 'poutcome': 4, 'age2': 78, 'balance2': 8590, 'day2': 31, 'duration2': 1824, 'campaign2': 52, 'pdays2': 628, 'previous2': 54}\n"
     ]
    }
   ],
   "source": [
    "# === NUM → CAT ===\n",
    "SIZES = {}\n",
    "\n",
    "num2cat_exprs = [\n",
    "    pl.col(c)\n",
    "    .cast(pl.Utf8)\n",
    "    .cast(pl.Categorical)\n",
    "    .to_physical()\n",
    "    .cast(pl.Int32).alias(f\"{c}2\")\n",
    "    for c in NUMS\n",
    "]\n",
    "\n",
    "num_df2 = all_data.select(num2cat_exprs)\n",
    "NUMS2 = num_df2.columns\n",
    "\n",
    "all_data = all_data.with_columns(cat_exprs + num2cat_exprs)\n",
    "\n",
    "SIZES = all_data.select(\n",
    "    [pl.col(col)\n",
    "     .n_unique()\n",
    "     .alias(col) for col in CATS + NUMS2]\n",
    ").to_dicts()[0]\n",
    "\n",
    "print(SIZES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "148b25a5-9598-49f4-b05b-a85b39a9a1cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T05:46:39.990908Z",
     "iopub.status.busy": "2025-10-05T05:46:39.990789Z",
     "iopub.status.idle": "2025-10-05T05:46:40.044302Z",
     "shell.execute_reply": "2025-10-05T05:46:40.043761Z",
     "shell.execute_reply.started": "2025-10-05T05:46:39.990899Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 120 new columns\n"
     ]
    }
   ],
   "source": [
    "# === 2Comboのペアを作成 ===\n",
    "pairs = list(combinations(CATS + NUMS2, 2))\n",
    "\n",
    "combo_exprs = [(pl.col(c1) * SIZES[c2] + pl.col(c2))\n",
    "               .alias(f\"{c1}_{c2}\") for c1, c2 in pairs]\n",
    "\n",
    "COMBO = [f\"{c1}_{c2}\" for c1, c2 in pairs]\n",
    "\n",
    "combo2_df = all_data.with_columns(combo_exprs)\n",
    "\n",
    "print(f\"Created {len(combo_exprs)} new columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4e8373c-4726-4d4c-b97d-8fc9cb39122b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T05:46:40.044797Z",
     "iopub.status.busy": "2025-10-05T05:46:40.044675Z",
     "iopub.status.idle": "2025-10-05T05:46:42.878535Z",
     "shell.execute_reply": "2025-10-05T05:46:42.878126Z",
     "shell.execute_reply.started": "2025-10-05T05:46:40.044784Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bacac4beee4e3e925fb43994ddf319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 136 new columns\n"
     ]
    }
   ],
   "source": [
    "# === Targetをoriginalのものにする ===\n",
    "tr_df = combo2_df[:len(train)]\n",
    "test_df = combo2_df[len(train):len(train)+len(test)]\n",
    "\n",
    "orig_df = combo2_df[len(train)+len(test):]\n",
    "orig_df = orig_df.with_columns(y_orig.alias(\"target\"))\n",
    "\n",
    "te_cols = CATS + NUMS2 + COMBO\n",
    "\n",
    "te_orig = target_encoding_orig(\n",
    "    tr_df,\n",
    "    test_df,\n",
    "    orig_df,\n",
    "    key_cols=te_cols,\n",
    "    alpha=ALPHA,\n",
    "    stats=(\"mean\",)\n",
    ")\n",
    "\n",
    "print(f\"Created {len(te_orig.columns)} new columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e6cfaff-fa85-46cc-bc19-b376163ced15",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T05:46:42.878968Z",
     "iopub.status.busy": "2025-10-05T05:46:42.878878Z",
     "iopub.status.idle": "2025-10-05T05:47:00.466045Z",
     "shell.execute_reply": "2025-10-05T05:47:00.465631Z",
     "shell.execute_reply.started": "2025-10-05T05:46:42.878961Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "830a83ed6af94fdda5d32423d2c1e98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041c529abd5748e5b08512015de18542",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44e81e6b8c114c838d7311561fd4599a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d595bf9156f04ee3941c9afbe9cea5e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a13eaed4f84acf920b146c952be4d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1508ce4cce44074962d4acec9e1c028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 136 new columns\n"
     ]
    }
   ],
   "source": [
    "# === Target Encoding ===\n",
    "tr_df = tr_df.with_columns(y_tr.alias(\"target\"))\n",
    "\n",
    "te_df = target_encoding(\n",
    "    tr_df,\n",
    "    test_df,\n",
    "    key_cols=te_cols,\n",
    "    stats=(\"mean\", ),\n",
    "    alpha=ALPHA,\n",
    ")\n",
    "\n",
    "print(f\"Created {len(te_df.columns)} new columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cebd30a8-afc6-4488-90cb-ed99650e42de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T05:47:00.466807Z",
     "iopub.status.busy": "2025-10-05T05:47:00.466702Z",
     "iopub.status.idle": "2025-10-05T05:47:02.155731Z",
     "shell.execute_reply": "2025-10-05T05:47:02.155314Z",
     "shell.execute_reply.started": "2025-10-05T05:47:00.466799Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d15bef0f6394a41acd56151199c5502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 136 new columns\n"
     ]
    }
   ],
   "source": [
    "# === Count Encoding\n",
    "combo2_df = combo2_df[:len(train) + len(test)]\n",
    "ce_cols = te_cols\n",
    "ce_dict = {f\"{col}_ce\": np.zeros(all_data.height) for col in ce_cols}\n",
    "\n",
    "for col in tqdm(ce_cols):\n",
    "    counts = combo2_df.group_by(col).agg(pl.len().alias(f\"{col}_ce\"))\n",
    "    joined_df = combo2_df.join(counts, on=col, how=\"left\")\n",
    "    ce_dict[f\"{col}_ce\"] = joined_df[f\"{col}_ce\"]\n",
    "\n",
    "ce_df = pl.DataFrame(ce_dict).with_columns([\n",
    "        pl.col(col).cast(pl.Float32) for col in ce_dict.keys()\n",
    "])\n",
    "\n",
    "print(f\"Created {len(ce_df.columns)} new columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c7a7650-7dca-46a3-9e82-907f62d041dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T05:47:02.156213Z",
     "iopub.status.busy": "2025-10-05T05:47:02.156106Z",
     "iopub.status.idle": "2025-10-05T05:47:02.161686Z",
     "shell.execute_reply": "2025-10-05T05:47:02.159979Z",
     "shell.execute_reply.started": "2025-10-05T05:47:02.156205Z"
    }
   },
   "outputs": [],
   "source": [
    "# === Dataの統合 ===\n",
    "all_data = pl.concat([\n",
    "    num_df,\n",
    "    te_df,\n",
    "    te_orig,\n",
    "    ce_df\n",
    "], how=\"horizontal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e3a1014-d56f-4c31-aadc-ac9feb200bdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T05:47:02.162750Z",
     "iopub.status.busy": "2025-10-05T05:47:02.162606Z",
     "iopub.status.idle": "2025-10-05T05:47:02.186714Z",
     "shell.execute_reply": "2025-10-05T05:47:02.186331Z",
     "shell.execute_reply.started": "2025-10-05T05:47:02.162740Z"
    }
   },
   "outputs": [],
   "source": [
    "# === row_id を追加 ===\n",
    "all_data = all_data.with_row_index(\"row_id\")\n",
    "\n",
    "# === Downcast ===\n",
    "all_data = downcast(all_data)\n",
    "\n",
    "# === データを分割 ===\n",
    "tr_df = all_data[:len(train)]\n",
    "test_df = all_data[len(train):len(train)+len(test)]\n",
    "\n",
    "# === targetを追加 ===\n",
    "tr_df = tr_df.with_columns(y_tr.alias(\"target\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f108bf5-e7c3-4971-a055-1080c161950b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T05:47:02.187119Z",
     "iopub.status.busy": "2025-10-05T05:47:02.187030Z",
     "iopub.status.idle": "2025-10-05T05:47:02.340106Z",
     "shell.execute_reply": "2025-10-05T05:47:02.339724Z",
     "shell.execute_reply.started": "2025-10-05T05:47:02.187111Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add Fold Col\n",
    "folds_path = \"../../artifacts/folds/folds.parquet\"\n",
    "pairs = [\n",
    "    (\"skf/k=5/s=42@train\", \"5fold-s42\")\n",
    "]\n",
    "cfgs = [c for c, _ in pairs]\n",
    "rename_map = {c: n for c, n in pairs}\n",
    "\n",
    "# folds をまとめて読み → ワイド化（cfg列を列見出しに）→ 列名をfold_nameにリネーム\n",
    "folds_wide = (\n",
    "    pl.scan_parquet(folds_path)\n",
    "      .filter(pl.col(\"cfg\").is_in(cfgs))\n",
    "      .unique(subset=[\"row_id\", \"cfg\"], keep=\"last\")\n",
    "      .select([\"row_id\", \"cfg\", \"fold\"])\n",
    "      .collect(engine=\"streaming\")\n",
    "      .pivot(values=\"fold\", index=\"row_id\", on=\"cfg\", aggregate_function=\"first\")\n",
    "      .rename(rename_map)\n",
    "      .with_columns(pl.col(\"row_id\").cast(pl.Int32))\n",
    "      .with_columns([pl.all().exclude(\"row_id\").cast(pl.Int8)])  # 型を軽く\n",
    ")\n",
    "\n",
    "# tr_df が DataFrame の場合\n",
    "tr_df = tr_df.join(folds_wide, on=\"row_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "515251e3-7de5-495a-8791-e499e9686986",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T05:47:02.340568Z",
     "iopub.status.busy": "2025-10-05T05:47:02.340431Z",
     "iopub.status.idle": "2025-10-05T05:47:02.345008Z",
     "shell.execute_reply": "2025-10-05T05:47:02.344553Z",
     "shell.execute_reply.started": "2025-10-05T05:47:02.340554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Shape & Memory ===\n",
      "Train Shape: (750000, 418), Test Shape: (250000, 416)\n",
      "Train Memory: 1.16 GB, Test Memory: 0.39 GB\n",
      "\n",
      "=== DTypes ===\n",
      "UInt32: 1\n",
      "Int32: 7\n",
      "Float32: 408\n",
      "Int8: 2\n"
     ]
    }
   ],
   "source": [
    "# === 特徴量エンジニアリング後の情報 ===\n",
    "train_mem, test_mem, n_cat = check_info(tr_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "526cdc6f-a8d1-42f6-b56f-0b8d640de52f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T05:47:02.345466Z",
     "iopub.status.busy": "2025-10-05T05:47:02.345360Z",
     "iopub.status.idle": "2025-10-05T05:47:06.404798Z",
     "shell.execute_reply": "2025-10-05T05:47:06.404339Z",
     "shell.execute_reply.started": "2025-10-05T05:47:02.345458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_df saved successfully to ../../artifacts/features/049/train.parquet\n",
      "test_df saved successfully to ../../artifacts/features/049/test.parquet\n"
     ]
    }
   ],
   "source": [
    "# === Save Overall Data ===\n",
    "tr_path = FEATURE_DIR / \"train.parquet\"\n",
    "test_path = FEATURE_DIR / \"test.parquet\"\n",
    "\n",
    "tr_df.write_parquet(tr_path)\n",
    "test_df.write_parquet(test_path)\n",
    "\n",
    "print(f\"tr_df saved successfully to {tr_path}\")\n",
    "print(f\"test_df saved successfully to {test_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d022ad-c3fa-4257-a39b-252ee38062a9",
   "metadata": {},
   "source": [
    "## Meta dataを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9057009d-3ca6-4fb1-84d6-b8fc3eda83b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-05T05:47:06.405437Z",
     "iopub.status.busy": "2025-10-05T05:47:06.405312Z",
     "iopub.status.idle": "2025-10-05T05:47:06.408905Z",
     "shell.execute_reply": "2025-10-05T05:47:06.408559Z",
     "shell.execute_reply.started": "2025-10-05T05:47:06.405428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_id: 049\n",
      "created_at: 2025-10-05T14:47:06.406648+09:00\n",
      "train_paths: ['../../artifacts/features/049/train.parquet']\n",
      "test_paths: ['../../artifacts/features/049/test.parquet']\n",
      "level: l1\n",
      "train_shape: [750000, 418]\n",
      "test_shape: [250000, 416]\n",
      "memory: {'train': 1.1636875569820404, 'test': 0.38743019104003906}\n",
      "fold_column: [('skf/k=5/s=42@train', '5fold-s42')]\n",
      "cat_cols: None\n"
     ]
    }
   ],
   "source": [
    "JST = timezone(timedelta(hours=9))\n",
    "meta = {\n",
    "    \"data_id\": ID,\n",
    "    \"created_at\": datetime.now(JST).isoformat(),\n",
    "    \"train_paths\": [str(tr_path)],\n",
    "    \"test_paths\": [str(test_path)],\n",
    "    \"level\": LEVEL,\n",
    "    \"train_shape\": [tr_df.height, tr_df.width],\n",
    "    \"test_shape\": [test_df.height, test_df.width],\n",
    "    \"memory\": {\n",
    "        \"train\": train_mem,\n",
    "        \"test\": test_mem\n",
    "    },\n",
    "    \"fold_column\": pairs,\n",
    "    \"cat_cols\": n_cat if n_cat else None\n",
    "}\n",
    "\n",
    "with open(f\"{FEATURE_DIR}/meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "for k, v in meta.items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a994a8b-06b8-4488-a31c-84b0b8fa961a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-25.08",
   "language": "python",
   "name": "rapids-25.08"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
