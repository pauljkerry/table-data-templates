# Kaggle Tabular Competition Templates ğŸ†

Kaggleã®ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ç”¨ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ãŠã‚ˆã³ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé›†ã§ã™ã€‚
GBDT, Neural Networks, ãã—ã¦ GPUåŠ é€Ÿã•ã‚ŒãŸ cuML ãƒ¢ãƒ‡ãƒ«ã‚’å«ã‚“ã§ã„ã¾ã™ã€‚

## ğŸ“¦ Supported Models

ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ã€‚

### Gradient Boosting Decision Trees (GBDT)
- **`xgb`**: XGBoost
- **`lgbm`**: LightGBM
- **`cb`**: CatBoost

### Neural Networks (Deep Learning)
- **`mlp`**: Multi-Layer Perceptron
- **`realmlp`**: RealMLP (ResNet-like architecture for tabular)
- **`tabnet`**: TabNet

### Classical Machine Learning (GPU Accelerated via cuML)
ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ã¯ `cuml` ã‚’ä½¿ç”¨ã—ã¦ãŠã‚Šã€**GPUç’°å¢ƒãŒå¿…é ˆ**ã§ã™ã€‚
- **`logreg`**: Logistic Regression
- **`rfr`**: Random Forest Regressor
- **`rfc`**: Random Forest Classifier
- **`ridge`**: Ridge Regression
- **`lasso`**: Lasso Regression
- **`svc`**: Support Vector Classifier

---

## âš ï¸ Important Usage Notes (å¿…ãšãŠèª­ã¿ãã ã•ã„)

ã“ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã™ã‚‹éš›ã¯ã€ä»¥ä¸‹ã®4ç‚¹ã‚’ã‚¿ã‚¹ã‚¯ã«åˆã‚ã›ã¦å¿…ãšä¿®æ­£ãƒ»ç¢ºèªã—ã¦ãã ã•ã„ã€‚

### 1. Data Preparation (`fold` column)
å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ã¯ã€CVï¼ˆCross Validationï¼‰ç”¨ã® **`fold` åˆ—ãŒå¿…é ˆ**ã§ã™ã€‚
äº‹å‰ã« StratifiedKFold ã‚„ GroupKFold ãªã©ã§ `fold` ã‚’å‰²ã‚ŠæŒ¯ã£ã¦ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã—ã¦ãã ã•ã„ã€‚


### 2. Adjust Objectives (Params)
å„ãƒ¢ãƒ‡ãƒ«ã®Trainerå†…ã«ã‚ã‚‹ params ã® objective (æå¤±é–¢æ•°) ã¯ã€ã‚¿ã‚¹ã‚¯ï¼ˆäºŒå€¤åˆ†é¡ã€å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ã€å›å¸°ãªã©ï¼‰ã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚

Binary Classification: binary:logistic, Logloss, etc.

Regression: reg:squarederror, RMSE, etc.

3. Adjust Metrics (Base CV Trainer)
BaseCVTrainer å†…ã§å®šç¾©ã•ã‚Œã¦ã„ã‚‹è©•ä¾¡æŒ‡æ¨™ï¼ˆMetricï¼‰ã‚‚ã‚¿ã‚¹ã‚¯ã«å¿œã˜ã¦å¤‰æ›´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚


# BaseCVTrainer or config
self.metric = ... # e.g., mean_squared_error, roc_auc_score
4. Hardware Requirement (GPU)
cuml ãƒ™ãƒ¼ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆSVC, Ridge, Lasso, RFãªã©ï¼‰ãŠã‚ˆã³ Deep Learning ãƒ¢ãƒ‡ãƒ«ã¯ GPUç’°å¢ƒ ã§ã®å®Ÿè¡Œã‚’å‰æã¨ã—ã¦è¨­å®šã•ã‚Œã¦ã„ã¾ã™ã€‚CPUç’°å¢ƒã§ã¯å‹•ä½œã—ãªã„ã€ã¾ãŸã¯è¨­å®šã®å¤‰æ›´ãŒå¿…è¦ã§ã™ã€‚

ğŸ›  Installation
å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ environment.yaml ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚ conda ç’°å¢ƒã‚’ä½œæˆã—ã¦ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

conda env create -f environment.yaml
conda activate <env_name>

# ğŸš€ Workflow
ã“ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ä»¥ä¸‹ã®é †åºã§å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’æƒ³å®šã—ã¦ã„ã¾ã™ã€‚

## 1. Feature Engineering
notebooks/fe/ ä»¥ä¸‹ã®Notebookã§ç‰¹å¾´é‡ã‚’ä½œæˆã—ã¾ã™ã€‚

Output: artifacts/features/{data_id}/train.parquet ãŠã‚ˆã³ meta.json

Rule: ä½œæˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã«ã¯å¿…ãšCVç”¨ã® fold åˆ—ã‚’å«ã‚ã¦ãã ã•ã„ã€‚

## 2. Hyperparameter Tuning (Optuna)
ãƒ¢ãƒ‡ãƒ«ã¨ä½œæˆã—ãŸãƒ‡ãƒ¼ã‚¿IDã‚’æŒ‡å®šã—ã¦Optunaã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

Output: artifacts/optuna/{model}-{data_id}/trl{n}.json

Note: ã“ã®JSONã«ã¯ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã ã‘ã§ãªãã€æ¢ç´¢æ™‚ã®ãƒ¡ã‚¿æƒ…å ±ã‚‚å«ã¾ã‚Œã¾ã™ã€‚å­¦ç¿’æ™‚ã«ã¯ã“ã“ã‹ã‚‰å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

## 3. Training & CV
notebooks/training/02_gpu_cv.ipynb (ã¾ãŸã¯å¯¾å¿œã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ) ã‚’ä½¿ç”¨ã—ã¦å­¦ç¿’ã‚’è¡Œã„ã¾ã™ã€‚ Optunaã§ç‰¹å®šã—ãŸ trl{n}.json ã‚’æŒ‡å®šã—ã¦ã€OOFãŠã‚ˆã³Testäºˆæ¸¬ã‚’ä½œæˆã—ã¾ã™ã€‚

Input: Feature (data_id), Params (trl{n}.json)

Output: runs/{model}-{data_id}-trl{n}-{fold}fold-s{seed}/

ã“ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« oof_pred, test_pred, ç‰¹å¾´é‡é‡è¦åº¦ã®ãƒ—ãƒ­ãƒƒãƒˆã€Notebookã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆç­‰ãŒä¿å­˜ã•ã‚Œã¾ã™ã€‚

## 4. Submission / Ensemble
notebooks/others/submission.ipynb ã«ã¦ã€runs/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã® test_pred (ã¾ãŸã¯ oof_pred ã‚’ä½¿ã£ãŸEnsembleçµæœ) ã‚’èª­ã¿è¾¼ã¿ã€æå‡ºç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚